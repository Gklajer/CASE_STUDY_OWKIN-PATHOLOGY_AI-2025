{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7ny-ukFRz02"
   },
   "source": [
    "# Case Study Owkin - Pathology AI - 2025\n",
    "\n",
    "## Context\n",
    "\n",
    "Cell segmentation in [hematoxylin and eosin (H&E)](https://www.cancer.gov/publications/dictionaries/cancer-terms/def/hematoxylin-and-eosin-staining) stained histopathology slides is a critical task in digital pathology, aiding in diagnosing diseases like cancer and quantifying key [biomarkers](https://en.wikipedia.org/wiki/Biomarker). Accurate segmentation enables automated workflows for identifying diverse cell types, improving consistency and efficiency in pathology assessments. However, the heterogeneity of tissue structures, staining variability, and presence of rare cell types make this task challenging. Developing robust models can enhance the precision of diagnoses, streamline workflows, and facilitate personalized medicine by providing actionable insights from tissue samples.\n",
    "\n",
    "## Accessing data\n",
    "\n",
    "For this challenge, you will have to work with the ConSep dataset. We have already preprocessed the dataset and tiled it for you. You can find it [here](https://www.kaggle.com/datasets/rftexas/tiled-consep-224x224px/). This dataset contains two subdirectories:\n",
    "\n",
    "1. `tiles`: contains all the raw `.png` images,\n",
    "2. `labels`: contains all the `*.mat` objects that store the ground truth labels.\n",
    "\n",
    "We already wrote the data loaders that fetch both the images and the ground truth labels from both subdirectories.\n",
    "\n",
    "We also provide the [weights](https://www.kaggle.com/models/afiltowk/phikon/pyTorch/default) of the Phikon backbone, which you will need for the model to run.\n",
    "\n",
    "## Accessing GPUs\n",
    "\n",
    "For this challenge, you will need access to GPUs to train your models. On the Kaggle Notebook interface, on the right-side bar, go to Session options > Accelerator, and set it to GPU P100. This will provide you with all the compute power you need to train a model. You will be granted 30 hours available for a week. **Beware of switching off your environment when you are done working !** Otherwise, GPU hours will be consumed.\n",
    "\n",
    "## Persistence of your environment\n",
    "\n",
    "To save files and outputs you may generate in your local environment at `/kaggle/working`, **we strongly advise you to set the persistence**. On the Kaggle Notebook interface, on the right-side bar, go to Session options > Persistence > Variables and Files. Otherwise, you will loose everything when relaunching your environment !\n",
    "\n",
    "## Task description\n",
    "\n",
    "Your task is to implement a training loop for a cell segmentation model called [CellViT](https://arxiv.org/abs/2306.15350).\n",
    "This model is fitted on the ConSep dataset, which contains 977 images with corresponding cell instance segmentation masks. We already provide the model architecture, the loss functions and the data loaders.\n",
    "\n",
    "We ask you to implement the training loop, to train the model and properly validate the model performance on the dataset. You will be evaluated on the clarity and efficiency of your implementation, evaluation procedure, and results analysis. We expect you to implement (at least explain) how you evaluated (would evaluate) the convergence of your model: metrics, visualization, etc.\n",
    "\n",
    "You can find below the structure of the package that we provide, and that contains helper functions including visualization scripts.\n",
    "\n",
    "```shell\n",
    ".\n",
    "├── __init__.py\n",
    "├── data\n",
    "│   ├── __init__.py\n",
    "│   ├── dataset.py\n",
    "│   ├── label.py\n",
    "│   ├── label_transform.py\n",
    "│   └── tiled_dataset.py\n",
    "├── loss\n",
    "│   ├── __init__.py\n",
    "│   ├── cellvit_loss.py\n",
    "│   ├── dice.py\n",
    "│   ├── focal.py\n",
    "│   ├── focal_tversky.py\n",
    "│   └── msge.py\n",
    "├── model\n",
    "│   ├── __init__.py\n",
    "│   ├── cellvit.py\n",
    "│   ├── decoder.py\n",
    "│   ├── extractor\n",
    "│   ├── neck.py\n",
    "│   └── utils.py\n",
    "├── postprocess\n",
    "│   ├── __init__.py\n",
    "│   └── instance_map.py\n",
    "└── viz\n",
    "    ├── __init__.py\n",
    "    ├── utils.py\n",
    "    └── visualize.py\n",
    "```\n",
    "\n",
    "**Important**: Take notes about what you're trying and what's working/not working so that we can understand your thought process.\n",
    "\n",
    "Once you have a baseline, we expect you to propose new ideas to improve the model performance on ConSep. It can be ideas in the literature or your own ideas. Bonus points will be granted if you manage to implement and evaluate your ideas.\n",
    "\n",
    "Among other ideas, you might want to explore the following but we don't expect you to spend too much time on it. Those are ranked by increasing complexity of implementation:\n",
    "\n",
    "1. How to make the training loop more efficient in terms of speed and / or GPU RAM?\n",
    "   \n",
    "2. How would you implement data augmentation? How would you implement it within the present code? You might want to copy paste code from `data/dataset::TrainingDataset` into your notebook to modify the training augmentations.\n",
    "   \n",
    "3. How would you use the bigger version of Phikon (ViT-B), which is [Phikon-v2](https://huggingface.co/owkin/phikon-v2) (ViT-Large)? How would you implement it and what are the pros / cons of using such model? You might want to copy-paste code from `model/extractor/extractor::PhikonViT` and try adapting it to Phikon-v2, which can be loaded using the `transformers` API (Hugging Face). You will find a code snippet to generate features for this model in the last section.\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "You have 7 days (maximum) to work on these different points. Nonetheless, do not spend too much time on it!\n",
    "We expect that this challenge should not take you more than 10 hours of cumulative work.\n",
    "\n",
    "Once you are finished, we ask you to share with us:\n",
    "\n",
    "1. This notebook with the code completed\n",
    "2. A 3-page report that details your understanding of the challenge, your thought process, ideas you tried and\n",
    "difficulties you faced (if any).\n",
    "\n",
    "## Bonus: How to use Phikon-v2\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "\n",
    "# Load an image\n",
    "image = Image.open(\n",
    "    requests.get(\n",
    "        \"https://github.com/owkin/HistoSSLscaling/blob/main/assets/example.tif?raw=true\",\n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "# Load phikon-v2\n",
    "processor = AutoImageProcessor.from_pretrained(\"owkin/phikon-v2\")\n",
    "model = AutoModel.from_pretrained(\"owkin/phikon-v2\")\n",
    "model.eval()\n",
    "\n",
    "# Process the image\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# Get the features\n",
    "with torch.inference_mode():\n",
    "    outputs = model(**inputs)\n",
    "    features = outputs.last_hidden_state[:, 0, :]  # (1, 1024) shape\n",
    "\n",
    "assert features.shape == (1, 1024)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from utils import fetch_kaggle_paths\n",
    "\n",
    "weight_path, data_path, helper_path = fetch_kaggle_paths()\n",
    "sys.path.append(helper_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01JEXQDKKM34WTHX53P3K1WTNX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-10T20:16:33.801099Z",
     "iopub.status.busy": "2024-12-10T20:16:33.800729Z",
     "iopub.status.idle": "2024-12-10T20:16:37.013966Z",
     "shell.execute_reply": "2024-12-10T20:16:37.012932Z",
     "shell.execute_reply.started": "2024-12-10T20:16:33.801067Z"
    },
    "id": "ttlJ_7fFRz04",
    "outputId": "1ae38d7c-28ae-4001-a7bc-b59c7d329946",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import torch\n",
    "from owkin_case_study.data.dataset import TrainDataset\n",
    "from owkin_case_study.data.tiled_dataset import TiledTrainingDataset\n",
    "from owkin_case_study.loss import CellViTCriterion\n",
    "from owkin_case_study.model import CellViT\n",
    "from owkin_case_study.model.extractor import PhikonViT\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import NUMBER_CELL_TYPES\n",
    "\n",
    "\n",
    "############################\n",
    "# We show below how to load the data and create data loaders that can be used to\n",
    "# train and validate your model. Feel free to change how to load your data, how\n",
    "# you validate the model. This is just an example.\n",
    "############################\n",
    "\n",
    "\n",
    "ddb_dataset = TiledTrainingDataset.from_tiled_datasets(\n",
    "    *[TiledTrainingDataset(root_path=data_path)]\n",
    ")\n",
    "\n",
    "tiles_path = Path(data_path) / \"tiles\"\n",
    "\n",
    "train_tiles = [x.stem for x in tiles_path.glob(\"train_*.png\")]\n",
    "test_tiles = [x.stem for x in tiles_path.glob(\"test_*.png\")]\n",
    "\n",
    "dataset_train, dataset_val = data.random_split(\n",
    "    TrainDataset(ddb_dataset, subset=train_tiles),\n",
    "    [\n",
    "        floor(0.8 * len(train_tiles)),\n",
    "        ceil(\n",
    "            0.2 * len(train_tiles),\n",
    "        ),\n",
    "    ],\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")\n",
    "dataset_test = TrainDataset(ddb_dataset, subset=test_tiles)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Dataset loaded! Found {len(train_tiles)} training/validation samples and {len(dataset_test)} test samples.\"\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset_train, batch_size=2, shuffle=True, pin_memory=True, num_workers=7\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset_val, batch_size=16, shuffle=False, pin_memory=True, num_workers=7\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset_test, batch_size=16, shuffle=False, pin_memory=True, num_workers=7\n",
    ")\n",
    "\n",
    "\n",
    "###########################\n",
    "\n",
    "backbone = PhikonViT(weights_path=weight_path)\n",
    "\n",
    "model = CellViT(backbone=backbone, number_cell_types=NUMBER_CELL_TYPES)\n",
    "\n",
    "criterion = CellViTCriterion(num_classes=NUMBER_CELL_TYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torch.nn.functional import l1_loss, mse_loss\n",
    "from torchmetrics.functional.classification import accuracy\n",
    "\n",
    "import wandb\n",
    "from lightning_model import LightningModel\n",
    "from utils import partial\n",
    "\n",
    "litmodel = LightningModel(\n",
    "    model,\n",
    "    criterion,\n",
    "    clf_keys={\"np\", \"tp\"},\n",
    "    reg_keys={\"hv\"},\n",
    "    metrics_dict=dict(\n",
    "        np=[partial(accuracy, task=\"multiclass\", num_classes=2)],\n",
    "        tp=[partial(accuracy, task=\"multiclass\", num_classes=NUMBER_CELL_TYPES)],\n",
    "        hv=[mse_loss, l1_loss],\n",
    "    ),\n",
    ")\n",
    "\n",
    "wandb.init(project=\"Case_study_Pathology_AI_2025\")\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=1,\n",
    "    logger=WandbLogger(),\n",
    "    callbacks=[ModelCheckpoint(monitor=\"val_loss\", mode=\"max\")],\n",
    "    accelerator=\"cpu\",\n",
    ")\n",
    "# trainer.validate(litmodel, val_loader)\n",
    "trainer.fit(litmodel, train_loader, val_loader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6069499,
     "sourceId": 9884162,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6141284,
     "sourceId": 10109129,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 170750,
     "modelInstanceId": 148282,
     "sourceId": 174183,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
